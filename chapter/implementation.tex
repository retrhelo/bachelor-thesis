\section{松散RISC-V多核访存架构实现}

上一章节具体描述了本课题对于松散RISC-V多核访存架构的总体设计。但这样的设计仍然停留在行为级别的
描述与建模上，距离具体的技术落地还存在着一定的距离。本章将会从具体实现的角度对所设计的模块进行
描述，并着眼于解决实现过程中所需要解决的技术要点，以及所遇到的技术难点。

\subsection{支持内存访问的缓存队列实现}

对Corundum缓存队列进行改造是本课题所面临的第一个技术关键点。本课题需要通过相关设计，使得作为
处理单元的网络处理器能够通过访存操作高效地访问存储于缓存队列上的数据。这样的要求使得缓存队列需要
同时具有随机访问内存能够通过地址进行数据访问的特点，也同时支持传统缓冲队列先进先出的数据行为。

\subsubsection{缓存队列实现的技术思路}

传统的缓存队列通常基于寄存器或是单口随机访问存储器实现。在设计中，寄存器组或是单口RAM为缓存队列
提供了存储单元，用于存放进入队列的数据单元。同时，缓存队列会使用两个寄存器作为当前队列的队首与
队尾指针。队首指针寄存器指向了缓存队列最靠前的数据单元在存储器上的地址，而队尾指针则指向了最靠后
数据单元在存储器上的地址。当进行入队与出队操作时，缓存队列可以通过队首指针寄存器与队尾指针寄存器
中所保存的地址对存储单元（寄存器或是RAM，这两种方案在时序上略有不同）进行访存，从而完成入队与出队
的操作。

从这样的设计中可以看出，为了能够存储队列中的元素，缓存队列内部存在着一块连续内存。在传统的缓存队列中，
这块内存由首指针寄存器与尾指针寄存器进行寻址操作。通过合理的设计，可以允许访存端口通过新增的端口，
一并对内存进行访问，由此即可实现外部对缓存队列的访存操作。为此，本课题的缓存队列可以在现有缓存队列
的基础上稍加修改，增加访存端口访问内存的逻辑，即可实现本课题所需要的功能。

\subsubsection{访存操作与出入队操作的写地址冲突问题}

无论是使用寄存器方案还是随机访问存储器，都存在着可以同时支持两组访存端口的解决方案。然而，采用
两组访存端口也必然会引入一个问题：如何解决因两组访存端口同时写同一个地址所面对的冲突？得益于
网络数据包处理的特殊应用场景，本课题中该问题得以被避开。尽管同时存在着用于数据包出入队的访存端口
以及用于网络处理器访问的访存端口，这两个端口在进行访存操作时地址却不会发生冲突。这是因为网络处理器
所访问的总是已经进入了缓存队列的数据包，而队尾指针指向的则永远是下一个待进入的数据包地址，因此这
两者之间也就不会发生写地址冲突。

\subsection{数据包请求调度器的实现}

\autoref{fig:design_controlpath}给出了系统的控制通路设计。而这其中最为重要的部件就是负责
对数据包请求进行调度与分发的调度器。本节将对数据包请求调度器的具体实现进行介绍，结合具体设计说明
数据包请求调度器是如何完成调度，实现与缓冲队列以及网络处理器之间的交互的。

\subsubsection{数据包请求通信机制的实现}

数据包请求被用于在缓存队列与调度器之间传递信息。当数据包进入队列时，缓存队列会像调度器发送数据包
处理请求，以便调度器唤醒网络处理器，开始对数据包进行处理；而当网络处理器处理完成后，调度器会向
缓存队列发送数据包请求回复，告知缓存队列该数据包已经处理完成，可以离开队列。

\autoref{tbl:design_req_signal}展示了一组数据包请求信号的信号格式。请求信号包含了数据包的
在缓存队列上的起始地址，以及一对握手信号。当数据包请求信号正确地完成握手时，缓存队列就成功地完成
了一次数据包请求的传输。数据包请求中的起始地址则可以通过缓冲队列的队首指针寄存器得到。

\csvgfig{impl_req_fifoside}{缓存队列至调度器方向数据包请求信号通路}{1.0}

\autoref{fig:impl_req_fifoside}展示了缓存队列到调度器方向的数据包请求信号通路。为了能够更加
清晰地展示核心逻辑，图中省略了实现中不相关逻辑信号，并对逻辑做了简化。s\_axis\_last与s\_axis\_ready
是数据包入队端口的AXI Stream总线。其中，s\_axis\_last信号用于表明当前数据是总线传输的最后一拍\cite{arm2011axi}，
Corundum使用该信号来标明数据包传输的结束。因此，收到s\_axis\_last信号就意味着当前数据包上的
数据已准备就绪，可以向调度器发送数据包处理请求。所以\autoref{fig:impl_req_fifoside}将
s\_axis\_last信号直接接到了req\_valid信号端口。req\_ready作为握手信号的另一部分，其由调度器
提供，用于表示调度器当前是否可以接收新的数据包处理请求。

pkt\_first寄存器信号用于标注当前拍是否表示全新的数据包传输。pkt\_first寄存器的维护同样与
s\_axis\_last信号有关。在Corundum使用s\_axis\_last信号表示上一个数据包传输结束的同时，该信号
同样也标明了新的数据包传输的开始。因此，在设计上选择将s\_axis\_last信号直接接入到pkt\_first
寄存器的数据端，并在每一次握手成功时对该寄存器进行更新。

first\_addr寄存器信号则用于保存当前数据包的起始地址。该寄存器的数据输入端直接与缓冲队列的队首
指针寄存器相连。当所接收到的数据为数据包的第一拍时，就可将队首指针寄存器fifo\_head的值更新到
first\_addr寄存器中，并在当前数据包接收完成后作为数据包请求信号发送给调度器。

当网络处理器完成了数据包的处理后，调度器会向缓存队列发送数据包请求回复。数据包请求回复信号的组成
与数据包请求信号的组成是一致的。\autoref{fig:impl_req_schedside}显示了简化后的实现逻辑。
因为数据包请求回复信号的处理过程有软件参与，所以数据包请求回复信号的实现逻辑会相对更为简单。

\csvgfig{impl_req_schedside}{调度器至缓存队列方向数据包请求回复信号通路}{1.0}

\autoref{fig:impl_req_schedside}展示了调度器至缓存队列方向的信号通路。当网络处理器完成了
对数据包的处理后，其会访问DOORBELL寄存器来告知调度器数据包已经处理完成。此时DOORBELL寄存器
被置1。考虑到调度器会同时管理多个网络处理器，因此使用Round Robin算法对多个控制器中的DOORBELL
信号进行调度（图中“RR”部分）生成选择信号Sel。通过选择信号Sel，调度器借由多路选择器在多个就绪的
控制器中选择一个，向缓存队列发送数据包请求回复信号。当数据包请求回复信号握手成功时，DOORBELL寄存器
便会被清零，等待下一个待处理的数据包到来。

\subsubsection{网络处理器控制逻辑的实现}

% 数据包请求调度器的另一个重要功能是实现对网络处理器的控制逻辑。在上一章中提到，调度器使用睡眠与唤醒
% 机制实现对网络处理器的控制。本节将对这一机制的实现原理进行描述。

数据包请求调度器的另一个重要功能是实现对网络处理器的控制逻辑。在上一章中提到，调度器使用睡眠与
唤醒机制实现了对网络处理器的控制。在具体的实现中，这一过程同时涉及了硬件上的逻辑实现与软件上的
运行时实现。在硬件上，通过时钟门控技术，通过精心设计的控制寄存器布局，为软件提供了可供操作的控制
接口；而在软件上，通过提供结合底层硬件的运行时设计，最终实现了对网络处理器的控制。本节将分别从
硬件设计与软件设计的角度出发，对网络处理器的控制逻辑进行描述，并阐述其主要的工作原理与核心思想。

\subsubsubsection{时钟门控技术}

网络处理器睡眠与唤醒的基本原理是被称为时钟门控（Clock Gating）的技术。这是一种常见于低功耗设计
的时序逻辑控制手段。该方案的要点在于通过控制逻辑，根据需要暂停或开启被控制模块的时钟信号，从而实现
对被控制模块的控制。

现代片上系统中有着大量的时序逻辑，而这些时序逻辑则均需要时钟进行驱动。因此片上系统中往往会存在
复杂的时钟通路。因为时钟通路由于其特性，总是呈现出树状结构，因此片上系统的时钟通路又被称为时钟树。
因为所有的时序逻辑均依赖时钟树进行驱动，因此时钟树是片上系统中最为重要的信号通路。良好的片上系统
设计对时钟树稳定性的要求极高。

\csvgfig{impl_ctrl_clkgating}{时钟门控电路原理图}{0.6}

而时钟门控技术则需要对被控制模块的时钟树进行修改。因此对于时钟门控电路的设计必须极为慎重。本课题
所使用的时钟门控电路主要来自于PsPIN项目\cite{di2020pspin}。PsPIN项目使用了一种简单而有效的
组合逻辑电路来实现时钟门控。通过锁存电路，该设计避免了门控过程中可能产生的毛刺，并保证了门控解除
时时钟信号能稳定地产生上升沿。

\subsubsubsection{控制寄存器设计}

\autoref{tbl:design_scheduler_ctrl}中给出了网络处理器控制逻辑中相关的控制寄存器。其中，
NP\_BUSY寄存器直接连接到时钟门控电路作为使能端对网络处理器进行时钟门控。而DOORBELL寄存器则作为
作为“门铃寄存器”供网络处理器访问。当网络处理器完成处理时，其会通过访存操作访问DOORBELL寄存器，
而当DOORBELL寄存器被访问后，其通过控制逻辑关闭NP\_BUSY寄存器，停止网络处理器的输入时钟，使
网络处理器进入睡眠。同时，DOORBELL寄存器还被用于标记数据包是否已经处理完成。\autoref{fig:impl_req_schedside}中
的逻辑便是使用了DOORBELL寄存器来判断数据包是否已经处理完成。

\subsubsubsection{网络处理器运行时}

运行时（Runtime）是用于在计算机上提供基本的执行环境的代码。其负责与底层软硬件交互，并向上层软件
提供一套可用的编程接口抽象。在本课题的应用内，因为网络处理器应用场景的特殊性，其需要独特的运行时
来保证网络处理器可以正确地完成相关功能。在本课题中，网络处理器运行时将负责完成如下功能。

\begin{enumerate}
  \item 设立内存栈，建立基本的C语言运行环境
  \item 从网络处理器控制逻辑中获取数据包首地址
  \item 建立栈帧，进入数据包处理逻辑
  \item 在数据包处理完成后访问网络处理器控制逻辑，标记数据包已处理完成
\end{enumerate}

网络处理器通过访存操作实现对网络处理器控制逻辑的访问。\autoref{tbl:design_scheduler_ctrl}展示
了控制逻辑上控制寄存器的相关地址。为了能够为上层应用的编写提供较好的抽象，网络处理器运行时应当完成
对相关控制寄存器的访问。基于对控制寄存器的访问结果，建立用于数据包处理逻辑的栈帧，从而实现底层逻辑
对数据包处理逻辑的抽象。

尽管网络处理器控制逻辑提供了用于时钟门控的相关逻辑，但时钟门控逻辑仍然需要知晓何时对网络处理器进行
控制。在上文已经提到，当网络处理器完成了对数据包的处理后，其将访问DOORBELL寄存器告知控制逻辑，
数据包已经处理完成。因此，网络处理器运行时应当在合适的时机访问位于网络处理器控制逻辑上的DOORBELL
寄存器。

\csvgfig{impl_runtime}{网络处理器运行时流程图}{0.8}

当最初的初始化完成后，网络处理器便总是处于“唤醒-处理数据包-睡眠”的循环之中。因此，网络处理器运行时
应当采用循环设计，以便网络处理器总是能够以相同的状态实现对网络数据包的处理。同时，考虑到网络处理器
在数据包处理完成后进入睡眠，而在新的数据包就绪时被唤醒。因此对DOORBELL的访问应当介于两次数据包
处理之间。

\subsubsection{轮询调度算法与数据包请求调度}

Round Robin算法又称轮询调度算法，是一种通过轮询访问进行请求调度的算法。该算法通过轮询的方式，
实现请求的分发与调度工作。该算法的优势在于其实现简单，且能够提供一定程度上的负载均衡，因此较为
适合被应用于本课题的应用场景。

本课题使用掩码和状态码实现Round Robin算法。网络处理器当前的状态构成了信号状态码state。同时，
使用掩码mask作为选择信号。掩码mask表示在当前选择中有效的选择范围。结合状态码state与掩码mask，
可以建立如下的状态转移公式

\begin{equation}
state_{masked} = state \land mask
\end{equation}
\begin{equation}
mask_{next} = \sim (state_{masked} \bigotimes (state_{masked} - 1))
\end{equation}

同时，结合state有选择信号sel

\begin{equation}
sel = state_{masked} \land \sim (state_{masked}-1)
\end{equation}

为了便于说明该机制的工作方式，不妨假设一段情景。假设现有4个网络处理器等待调度，则这些网络处理器的
状态构成了4位的信号状态码。同时，使用4位的掩码对状态码进行筛选。\autoref{tbl:impl_roundrobin}展示了
经过四轮调度后状态码与掩码的状态。

\begin{generaltab}{使用轮询调度算法进行调度}{tbl:impl_roundrobin}
  \begin{tabular}{c|cccc}
    \toprule
    调度次数 & $state$ & $mask$ & $state_{masked}$ & $sel$ \\
    \midrule
    1 & 4'b1011 & 4'b1111 & 4'b1011 & 4'b0001 \\
    2 & 4'b1010 & 4'b1110 & 4'b1010 & 4'b0010 \\
    3 & 4'b1000 & 4'b1100 & 4'b1000 & 4'b1000 \\
    4 & 4'b0000 & 4'b0000 & 4'b0000 & 4'b0000 \\
    \bottomrule
  \end{tabular}
\end{generaltab}

注意到，经过四轮调度后，此时掩码变为了$4'b0000$，无法再被继续用于调度。因此，在以上逻辑计算的
基础上，可以加上一段选择判断电路，使得当$mask_{next}$等于0时，将$mask$的值替换为全1。这样就
允许电路逻辑在完成一轮调度后再进行下一轮调度了。

\csvgfig{impl_roundrobin}{轮询调度算法电路示意图}{0.9}

基于上述原理，可以很容易地设计出数字电路用于实现轮询调度算法。\autoref{fig:impl_roundrobin}展示了
在本课题中这一电路的大致逻辑。通过这样的设计，来自网络处理器的状态被编码为统一的状态信号，经过
轮询调度器，产生选择信号。选择信号将会被用于多路选择器，以便从多个网络处理器中选择出一个用于响应
网络数据包请求。

\subsection{带缓存的Picorv32处理器实现}

作为真正负责进行网络数据包处理的网络处理器而言，Picorv32在架构中扮演着举足轻重的角色。因此，
Picorv32在架构中的性能表现也就变得尤为重要。然而，处于面积与时序上的优化考虑，Picorv32牺牲了
不少的性能\cite{picorv32}。这使得其在性能上的表现显得难以让人满意。与大多数的处理器不同，Picorv32
并未采用流水线设计，相反其采用了更为简单的多周期处理器设计。这使得Picorv32的CPI——即是是在理想
情况下——也往往能达到3个周期及以上。

与此同时，Picorv32所采用的访存系统也同样存在着较大的优化空间。出于设计的简单考虑，Picorv32处理器
并未使用缓存设计，而是使用了被称为Picorv32 Native Memory Bus\cite{picorv32}的自定义访存
总线用于基本的访存操作。在此基础上，为了方便集成，Picorv32还提供了支持AXI Lite总线的顶层实现。

\begin{generaltab}{Picorv32自定义访存总线\cite{picorv32}}{tbl:impl_picorv32_nativebus}
  \begin{tabular}{cccc}
    \toprule
    信号名称 & 信号位宽 & 信号方向 & 信号说明 \\
    \midrule
    mem\_valid & 1 & output & 握手信号 \\
    mem\_instr & 1 & output & 是否为取指操作 \\
    mem\_ready & 1 & input & 握手信号 \\
    mem\_addr & 32 & output & 访存地址信号 \\
    mem\_wdata & 32 & output & 写数据信号 \\
    mem\_wstrb & 4 & output & 写掩码信号 \\
    mem\_rdata & 32 & input & 读数据信号 \\
    \bottomrule
  \end{tabular}
\end{generaltab}

缺少缓存对于Picorv32的访存性能的影响是非常严重的。考虑到现代访存架构中对于片外存储设备的访问往往
速度缓慢，需要大量的周期完成，因此在缺少缓存的情况下，Picorv32不得不花费上百甚至上千个周期来完成！
这对于追求速度的网络数据包处理应用而言显然是无法接受的。因此，为了得到较好的性能表现，有必要在
Picorv32的基础上增加缓存系统。

\subsubsection{缓存架构的实现}

得益于现有的开源项目，本课题并不需要从零开始实现一套缓存系统。名为IOB-Cache的开源工作\cite{roque2021iob}为
本课题提供了可行的开源实现。IOB-Cache是目前不多的开源缓存项目之一，其提供了良好的可配置性，与较好
的访存性能。同时，IOB-Cache还提供了性能优异的采用“写穿”策略的缓存与控制逻辑设计。这样的设计使得
IOB-Cache可以提供开销接近于一个周期的近乎理想的写内存操作。于此同时，IOB-Cache还提供了现有的
集成SoC环境可供参考，在该集成设计中，CPU的指令平均周期数（CPI）指标最好能够达到1.055，非常接近
于理想情况下的CPU实现。

同时，IOB-Cache在接口上与Picorv32采用了几乎一致的接口。尽管其接口与Picorv32的访存接口在时序
上存在些许差别，但在实现上仍然可以较为轻松地完成Picorv32与IOB-Cache的连接。在\autoref{section:design_module}中，
由\autoref{fig:pico_no_cache}、\autoref{fig:pico_single_cache}以及\autoref{fig:pico_dual_cache}分别
给出了三种不同的缓存架构设计设计思路，同时也讨论了采用不同缓存架构所带来的设计上的好处与坏处。综合
前文的考量，本课题认为单块缓存的设计是最为符合本课题需求的：该设计预期将有着相对较好的访存性能表现，
于此同时，该实现相对较为简单，对时序与面积的影响也相对较小。

\autoref{fig:pico_single_cache}已经给出了较为具体的架构设计，因此缓存架构在实现上的主要工作
为连接与集成所用到的各个IP。下文将逐步介绍各个IP在架构中所扮演的角色，并对实现思路进行解释。

IOB-Cache与Picorv32均使用了被称为Native Memory Bus的自定义总线作为其基础的访存总线方案。
这使得在实现中可以容易地将IOB-Cache的前端与Picorv32通过总定义总线连接起来。\autoref{tbl:impl_picorv32_nativebus}展示了
该总线的信号构成。通过一对握手信号，该总线可以通过一次握手中完成读数据操作或是写数据操作。

简单地自定义总线也为多路选择器的实现提供了便利。为了实现不经过缓存的旁路功能，需要对由处理器核引出
的访存总线进行解码与分发。出于设计简单的考虑，在实现中选择使用地址总线的最高位作为是否经过缓存的
判断信号。当地址总线最高位为1时，访存操作将通过缓存；反之则经由旁路。\autoref{tbl:impl_picorv32_nativebus}中的UseCache信号
也由此产生。

cache\_axi是IOB-Cache所提供的一种实现。该实现选择使用AXI总线作为缓存后端，连接外部的存储系统。
但在本课题中，主要采用AXI Lite总线作为访存系统的通用总线。因此，对于IOB-Cache的缓存后端而言，
需要使用AXI到AXI Lite的总线桥对总线信号进行转换，以便能够正常地连接到外部的存储设备。同理，旁路
上则使用了Picorv32项目自带的AXI Lite总线转接模块进行连接。

缓存通路与旁路上的AXI Lite总线信号最终将通过多路选择器，由UseCache信号进行选择，经处理后与外部
的存储系统相连接。

\subsection{片外存储设备的访问实现}

片外存储设备的访问将主要通过AMBA总线完成。Alveo U280数据中心加速卡提供了丰富的片外存储资源可供
使用，包括总容量达32GB的DDR存储器，以及8GB大小的HBM2高带宽存储器\cite{alveo2021datasheet}。通常，
片外存储设备通过引脚与片内的存储控制器设计相连。片内逻辑访问片外存储设备可以通过访问位于片内的
存储控制器完成。Vivado设计工具组针对Alveo U280提供了完善的片内控制器IP可供使用。片内控制器IP
通常支持如AXI总线等常见的AMBA总线协议。因此，本课题可以容易地使用AXI Lite总线协议实现对片外存储
设备进行访问。

\subsection{本章小节}

本章主要对松散RISC-V多核访存架构的实现细节进行了描述。沿着数据包在数据通路上传输的顺序，本章首先
对支持内存访问的缓存队列实现进行了介绍，说明了本课题如何在Corudum缓存队列的基础上添加内存访问支持。
然后，作为设计的重点，本章着重描述了数据包请求调度器与带缓存的Picorv32处理器的实现。通过这些描述，
本章详细地介绍了本设计如何通过控制通路与访存架构对网络数据包进行处理。最后，本章对Alveo U280数据
中心加速卡上访问片外存储设备的方式进行了介绍。
