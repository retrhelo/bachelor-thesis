@article{di2020pspin,
  title={PsPIN: A high-performance low-power architecture for flexible in-network compute},
  author={Di Girolamo, Salvatore and Kurth, Andreas and Calotoiu, Alexandru and Benz, Thomas and Schneider, Timo and Ber{\'a}nek, Jakub and Benini, Luca and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2010.03536},
  year={2020}
}
@inproceedings{forencich2020corundum,
  title={Corundum: An open-source 100-gbps nic},
  author={Forencich, Alex and Snoeren, Alex C and Porter, George and Papen, George},
  booktitle={2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  pages={38--46},
  year={2020},
  organization={IEEE}
}
@article{maxiaoxiao2022survey,
  title={智能网卡综述},
  author={马潇潇 and 杨帆 and 王展 and 元国军 and 安学军},
  journal={计算机研究与发展},
  year={2022}
}
@online{bhalgat2021smartnic,
  title={Choosing the Best SmartNIC},
  author={Bhalgat, Ash},
  year={2021},
  url={https://developer.nvidia.com/blog/choosing-the-best-dpu-based-smartnic/}
}
@online{pcmag_smartnic,
  title={SmartNIC},
  publisher={PC Magazine},
  url={https://www.pcmag.com/encyclopedia/term/smartnic}
}
@inproceedings{firestone2018azure,
author = {Firestone, Daniel and Putnam, Andrew and Angepat, Hari and Chiou, Derek  and Caulfield, Adrian and Chung, Eric and Humphrey, Matt and Ovtcharov, Kalin and Padhye, Jitu and Burger, Doug and Maltz, Dave and Greenberg, Albert and Mundkur, Sambhrama and Dabagh, Alireza and Andrewartha, Mike and Bhanu, Vivek and Chandrappa, Harish Kumar and Chaturmohta, Somesh and Lavier, Jack and Lam, Norman and Liu, Fengfen and Popuri, Gautham and Raindel, Shachar and Sapre, Tejas and Shaw, Mark and Silva, Gabriel and Sivakumar, Madhan and Srivastava, Nisheeth and Verma, Anshuman and Zuhair, Qasim and Bansal, Deepak and Vaid, Kushagra and Maltz, David A.},
title = {Azure Accelerated Networking: SmartNICs in the Public Cloud},
year = {2018},
month = {April},
abstract = {Modern cloud architectures rely on each server running its own networking stack to implement policies such as tunneling for virtual networks, security, and load balancing. However, these networking stacks are becoming increasingly complex as features are added and as network speeds increase. Running these stacks on CPU cores takes away processing power from VMs, increasing the cost of running cloud services, and adding latency and variability to network performance.

We present Azure Accelerated Networking (AccelNet), our solution for offloading host networking to hardware, using custom Azure SmartNICs based on FPGAs. We define the goals of AccelNet, including programmability comparable to software, and performance and efficiency comparable to hardware. We show that FPGAs are the best current platform for offloading our networking stack as ASICs do not provide sufficient programmability, and embedded CPU cores do not provide scalable performance, especially on single network flows.

Azure SmartNICs implementing AccelNet have been deployed on all new Azure servers since late 2015 in a fleet of >1M hosts. The AccelNet service has been available for Azure customers since 2016, providing consistent <15μs VM-VM TCP latencies and 32Gbps throughput, which we believe represents the fastest network available to customers in the public cloud. We present the design of AccelNet, including our hardware/software co-design model, performance results on key workloads, and experiences and lessons learned from developing and deploying AccelNet on FPGA-based Azure SmartNICs.},
url = {https://www.microsoft.com/en-us/research/publication/azure-accelerated-networking-smartnics-public-cloud/},
edition = {15th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
}
@inproceedings{fowers2015a,
author = {Fowers, Jeremy and Kim, Joo-Young and Burger, Doug and Hauck, Scott},
title = {A Scalable High-Bandwidth Architecture for Lossless Compression on FPGAs},
year = {2015},
month = {May},
abstract = {Data compression techniques have been the subject of intense study over the past several decades due to exponential increases in the quantity of data stored and transmitted by computer systems. Compression algorithms are traditionally forced to make tradeoffs between throughput and compression quality (the ratio of original file size to compressed file size). FPGAs represent a compelling substrate for streaming applications such as data compression thanks to their capacity for deep pipelines and custom caching solutions. Unfortunately, data hazards in compression algorithms such as LZ77 inhibit the creation of deep pipelines without sacrificing some amount of compression quality. In this work we detail a scalable fully pipelined FPGA accelerator that performs LZ77 compression and static Huffman encoding at rates up to 5.6 GB/s. Furthermore, we explore tradeoffs between compression quality and FPGA area that allow the same throughput at a fraction of the logic utilization in exchange for moderate reductions in compression quality. Compared to recent FPGA compression studies, our emphasis on scalability gives our accelerator a 3.0x advantage in resource utilization at equivalent throughput and compression ratio.},
publisher = {IEEE – Institute of Electrical and Electronics Engineers},
url = {https://www.microsoft.com/en-us/research/publication/a-scalable-high-bandwidth-architecture-for-lossless-compression-on-fpgas/},
edition = {The 23rd IEEE International Symposium on Field-Programmable Custom Computing Machines},
}
@inproceedings{caulfield2016a,
author = {Caulfield, Adrian and Chung, Eric and Putnam, Andrew and Angepat, Hari and Fowers, Jeremy and Haselman, Michael and Heil, Stephen and Humphrey, Matt and Kaur, Puneet and Kim, Joo-Young and Lo, Daniel and Massengill, Todd and Ovtcharov, Kalin and Papamichael, Michael and Woods, Lisa and Lanka, Sitaram and Chiou, Derek and Burger, Doug},
title = {A Cloud-Scale Acceleration Architecture},
year = {2016},
month = {October},
abstract = {Hyperscale datacenter providers have struggled to balance the growing need for specialized hardware (efficiency) with the economic benefits of homogeneity (manageability).  In this paper we propose a new cloud architecture that uses reconfigurable logic to accelerate both network plane functions and applications.  This Configurable Cloud architecture places a layer of reconfigurable logic (FPGAs) between the network switches and the servers, enabling network flows to be programmably transformed at line rate, enabling acceleration of local applications running on the server, and enabling the FPGAs to communicate directly, at datacenter scale, to harvest remote FPGAs unused by their local servers.

We deployed this design over a production server bed, and show how it can be used for both service acceleration (Web search ranking) and network acceleration (encryption of data in transit at high-speeds).  This architecture is much more scalable than prior work which used secondary rack-scale networks for inter-FPGA communication.  By coupling to the network plane, direct FPGA-to-FPGA messages can be achieved at comparable latency to previous work, without the secondary network.  Additionally, the scale of direct inter-FPGA messaging is much larger.  The average round-trip latencies observed in our measurements among 24, 1000, and 250,000 machines are under 3, 9, and 20 microseconds, respectively.   The Configurable Cloud architecture has been deployed at hyperscale in Microsoft's production datacenters worldwide.},
publisher = {IEEE Computer Society},
url = {https://www.microsoft.com/en-us/research/publication/configurable-cloud-acceleration/},
edition = {Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture},
}
@online{mellanox2020whitepaper,
  title={Innova series SmartNIC white paper},
  publisher={Mellanox},
  year={2020},
  month={08},
  day={10},
  url={https://www.mellanox.com/sites/default/files/doc-2020/pb-innova-2-flex.pdf}
}
@online{intel2020pac,
  title={The FPGA SmartNIC for network acceleration PAC N3000},
  publisher={Intel},
  year={2020},
  url={https://www.intel.com/content/www/us/en/programmable/products/boards_and_kits/dev-kits/altera/intel-fpga-pac-n3000/overview.html}
}
@online{intel2019d5005,
  title={Intel FPGA programmable acceleration card D5005 data sheet},
  publisher={Intel},
  year={2019},
  url={https://www.intel.com/content/www/us/en/programmable/documentation/cvl1520030638800.html}
}
@online{xilinx2020alveou25,
  title={Alveo U25 SmartNIC accelerator card},
  publisher={Xilinx},
  year={2020},
  url={https://www.xilinx.com/publications/product-briefs/alveo-u25-product-brief.pdf}
}
@online{xilinx2020x2,
  title={X2 series Ethernet adapters},
  publisher={Xilinx},
  year={2020},
  url={https://www.xilinx.com/products/boards-and-kits/x2-series.html}
}
@online{mellanox2020bluefield,
  title={Mellanox BlueField series SmartNIC white paper},
  publisher={Mellanox},
  year={2020},
  url={https://www.mellanox.com/files/doc-2020/pb-bluefield-2-smartnic-vpi.pdf}
}
@online{broadcom2020stingray,
  title={Stingray series SmartNIC},
  publisher={Broadcom},
  year={2020},
  url={https://www.broadcom.com/products/ethernet-connectivity/smartnic}
}
@online{annapurna2020announce,
  title={Announces availability of home network and storage platform-on-chip and subsystem solution},
  publisher={Annapurna Labs},
  year={2020},
  url={https://www.annapurnalabs.com/annapurna-labs-an-amazon-company-announces-availability-of-home-network-and-storage-platform-on-chip-and-subsystem-solutions.html}
}
